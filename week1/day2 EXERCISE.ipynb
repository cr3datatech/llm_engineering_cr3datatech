{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Creation**: Generative AI can generate high-quality content such as images, videos, and text, reducing the need for human writers and designers. This is particularly useful for companies that need to produce a large volume of content, such as e-commerce websites, marketing agencies, and media outlets.\n",
      "2. **Product Design and Prototyping**: Generative AI can design and prototype new products quickly and efficiently, saving time and resources for product development teams. This is useful for companies in industries such as aerospace, automotive, and consumer goods.\n",
      "3. **Marketing and Advertising**: Generative AI can generate personalized marketing messages, product descriptions, and ad copy, improving the effectiveness of marketing campaigns. It can also analyze customer behavior and preferences to create targeted ads.\n",
      "4. **Customer Service**: Generative AI-powered chatbots can provide 24/7 customer support, answering common questions and resolving simple issues, freeing up human agents for more complex problems.\n",
      "5. **Data Analysis and Visualization**: Generative AI can generate insights from large datasets, identifying patterns and trends that may not be apparent to humans. It can also create interactive visualizations to present these findings in a clear and concise manner.\n",
      "6. **Cybersecurity**: Generative AI can analyze network traffic and identify potential security threats in real-time, helping companies detect and prevent cyber attacks more effectively.\n",
      "7. **Supply Chain Optimization**: Generative AI can optimize supply chain operations by predicting demand, managing inventory levels, and identifying the most efficient routes for delivery trucks.\n",
      "8. **Financial Analysis and Modeling**: Generative AI can analyze large datasets of financial transactions to identify trends and patterns, helping companies make more informed investment decisions.\n",
      "9. **Healthcare**: Generative AI can analyze medical images, identify potential health risks, and develop personalized treatment plans.\n",
      "10. **Education**: Generative AI can create personalized learning plans for students, tailor educational content to individual learning styles, and grade assignments automatically.\n",
      "11. **Music and Audio Generation**: Generative AI can generate music, sound effects, and audio content, reducing the need for human composers and audio engineers.\n",
      "12. **Fashion Design**: Generative AI can design new fashion trends, patterns, and accessories, saving time and resources for fashion designers and brands.\n",
      "\n",
      "Some companies that are already leveraging Generative AI include:\n",
      "\n",
      "* Google (Generative AI-powered chatbots)\n",
      "* IBM (Generative AI-powered customer service)\n",
      "* Microsoft (Generative AI-powered content creation)\n",
      "* Amazon (Generative AI-powered product design and prototyping)\n",
      "* Spotify (Generative AI-powered music generation)\n",
      "\n",
      "As the technology continues to evolve, we can expect to see even more innovative business applications of Generative AI.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Artificial Intelligence (AI) has numerous business applications across various industries. Here are some examples:\n",
      "\n",
      "1. **Content Creation**: Generative AI can generate high-quality content such as images, videos, music, and text. This technology can be used by businesses to create engaging content for their websites, social media, and marketing campaigns.\n",
      "2. **Chatbots and Customer Service**: Generative AI-powered chatbots can help businesses provide 24/7 customer support, respond to customer inquiries, and resolve issues quickly. This can lead to improved customer satisfaction and reduced response times.\n",
      "3. **Product Design and Development**: Generative AI can be used to design and develop new products, such as furniture, clothing, and electronics. This technology can help businesses reduce their product development time and costs while maintaining high-quality designs.\n",
      "4. **Supply Chain Optimization**: Generative AI can analyze large datasets to identify patterns and optimize supply chain operations. This can lead to reduced costs, improved efficiency, and better inventory management.\n",
      "5. **Market Research and Analysis**: Generative AI can help businesses analyze market trends, customer preferences, and competitor activity. This information can be used to inform business decisions, such as product development, pricing, and marketing strategies.\n",
      "6. **Financial Modeling and Forecasting**: Generative AI can be used to build financial models that forecast revenue, expenses, and cash flow. This technology can help businesses make more informed investment decisions and reduce the risk of financial errors.\n",
      "7. **Marketing Automation**: Generative AI can automate marketing campaigns, such as email marketing, social media advertising, and content promotion. This can lead to improved campaign efficiency, reduced costs, and increased conversion rates.\n",
      "8. **Data Analysis and Visualization**: Generative AI can help businesses analyze large datasets and visualize complex information in a more intuitive and accessible way. This can lead to better insights and decision-making.\n",
      "9. **Recommendation Systems**: Generative AI can be used to build recommendation systems that suggest products, services, or content to customers based on their preferences and behavior.\n",
      "10. **Cybersecurity**: Generative AI can help businesses detect and respond to cyber threats more effectively by analyzing network traffic, identifying patterns, and predicting potential attacks.\n",
      "\n",
      "Some notable companies that are leveraging generative AI for business applications include:\n",
      "\n",
      "* Microsoft (AI-powered chatbots)\n",
      "* Amazon (AI-powered product recommendations)\n",
      "* IBM (AI-powered supply chain optimization)\n",
      "* Netflix (AI-powered content recommendation)\n",
      "* H&M (AI-powered fashion design)\n",
      "\n",
      "These examples illustrate the vast potential of generative AI to transform various aspects of business operations and drive growth, efficiency, and innovation.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including but not limited to:\n",
      "\n",
      "1. **Content Generation**: Create high-quality content such as blog posts, product descriptions, social media posts, and more with minimal human intervention.\n",
      "2. **Marketing Automation**: Use generative AI to personalize marketing campaigns, create targeted ads, and optimize email sequences for better ROI.\n",
      "3. **Product Design**: Generate design concepts, prototypes, and 2D/3D models for products, furniture, or architecture using tools like product design software and generative modeling technologies.\n",
      "4. **Language Translation**: Create machine translation systems that can accurately translate text into multiple languages with improved translation outcomes.\n",
      "5. **Chatbots and Virtual Assistants**: Develop conversational AI systems that learn from user interactions to offer personalized support and improve customer experience.\n",
      "6. **Image and Video Generation**: Create high-quality images or videos using GANs (Generative Adversarial Networks), for example, product renders, commercial photoshoots, or animated graphics.\n",
      "7. **Music Composition**: Use generative AI algorithms to create original music tracks based on specific styles, moods, or themes.\n",
      "8. **Data Analysis and Visualization**: Apply data-generating techniques like synthetic data generation or generative modeling to improve the accuracy of predictive models.\n",
      "9. **Product Development**: Collaborate with humans to generate new product ideas using various design tools and methods such as AI-assisted co-design.\n",
      "\n",
      "In general, Generative AI can help businesses across industries by: \n",
      "\n",
      "* Enhancing operational efficiency\n",
      "* Increasing productivity \n",
      "* Improving decision making capabilities\n",
      "* Developing high-quality content at a lower cost\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \u001b[K\n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \u001b[K\n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \u001b[K\n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to explain certain key concepts behind Large Language Models (LLMs), specifically focusing on what neural networks are, attention, and transformers. Hmm, where do I start?\n",
      "\n",
      "First, let's break down the user's query. They asked for definitions of these concepts behind LLMs: a neural network, attention, and the transformer. So, they likely want clear explanations that someone without prior knowledge could understand.\n",
      "\n",
      "Starting with the neural network part. From what I remember, a neural network is inspired by the human brain. It has layers of nodes or neurons processing information. The key idea here is that these networks learn through data examples, adjusting their connections automatically to better predict tasks like image recognition or language translation. So maybe I should mention how neural networks process information, perhaps including layers with different functions and activation rules.\n",
      "\n",
      "Next up is attention, which is a part of the transformer architecture. I know that traditional approaches look only at previous tokens when processing the next token, but transformers use something called 'scaled dot-product attention.' This seems to focus on local parts of the sequence based on their relative positions. So maybe explain how instead of looking back all the time, the model examines specific nearby tokens related to the current one.\n",
      "\n",
      "Then there's the transformer itself. I think it uses self-attention mechanisms in a distributed way but isn't a standard RNN or CNN per se. It processes input sequences in parallel, which allows it to handle variable-length contexts dynamically and focus softly on relevant parts of the text at any given time. So maybe clarify that transformers process data batch-wise, each token being treated independently for attention.\n",
      "\n",
      "I should make sure my language is clear without too much jargon. Use analogies where appropriate, like comparing neural networks as something smart but mysterious with layers working together without knowing all connections upfront. For attention and transformers, highlight the parts of how they get info from different parts of the input sequence during processing.\n",
      "\n",
      "Wait, I should structure this explanation so each concept is explained separately, perhaps in a numbered format under headings. Also, maybe include brief examples or analogies to make it clearer for someone new. But since I'm writing this without any prior reading knowledge, keeping explanations straightforward and concise will help.\n",
      "\n",
      "I think that's about it. Let me put it all together now.\n",
      "</think>\n",
      "\n",
      "### Core Concepts Behind Large Language Models (LLMs)\n",
      "\n",
      "#### 1. How Neural Networks Work:\n",
      "Neural networks, as inspired by the human brain, are mathematical models designed to simulate how neurons in the brain function. They consist of layers of interconnected nodes or \"neurons\" that process information and pass it along through connections weighted with certain values. Each neuron applies an activation function to a linear combination of its inputs before passing the result through the function. Through layers (e.g., input, hidden, output), neural networks learn representations of data from examples, adjusting their connection weights based on prediction errors.\n",
      "\n",
      "In simpler terms:\n",
      "- **Layers:** Data is processed in multiple layers where each layer transforms and combines data.\n",
      "  - **Input Layer:** receives raw data.\n",
      "  - **Hidden Layers:** Perform computations to extract features.\n",
      "  - **Output Layer:** Uses learned patterns to generate outputs, like translating text or recognizing images.\n",
      "- **Weighted Connections:** Nodes in one layer connect to nodes in another with weights that adjust during training. These connections are initially random and learn based on data.\n",
      "- **Activation Functions:** Non-linearities (like ReLU or sigmoid) introduce non-linearity to handle complex patterns.\n",
      "\n",
      "**Neural networks process information through layers, adjusting their connections dynamically based on data examples.**\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. How Attention Works:\n",
      "Attention is a key component of the Transformer architecture, introduced by Vaswani et al. Originally, attention focused only on previous tokens when processing each token. Now, transformers use \"scaled dot-product attention,\" allowing the model to consider multiple nearby tokens when paying attention to a specific position in the input sequence.\n",
      "\n",
      "In simpler terms:\n",
      "- **Self-Augmented Attention:** Each token learns to attend to different parts of its context through shared representation.\n",
      "  - Example: When processing token \\( t \\), it looks at positions \\( (t+1) \\), \\( (t+k) \\), and so on, each focusing on a part of the sequence relevant to the task.\n",
      "- **Local Focus:** Transformer models don’t focus on all tokens; instead, they consider local context based on relative positions. This reduces memory requirements but enhances the model’s ability to attend to information from different parts.\n",
      "\n",
      "**Attention allows the model to look at specific parts of the input around a given token, focusing attention during processing.**\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. How Transformers Work:\n",
      "The Transformer breaks inputs into multiple parallel sequences and processes them in parallel using self-attention mechanisms and feed-forward neural networks. This distributed approach enables handling variable-length contexts dynamically.\n",
      "\n",
      "In simpler terms:\n",
      "- **Parallel Processing:** Treats each token in the sequence independently.\n",
      "  - Each token is treated as a single element without context from others yet, but attention allows it to access related parts during processing.\n",
      "- **Self-Aggregation:** Tokens learn representations by attending differently to information from other tokens. The self-attention mechanism enables soft focusing (attending softly) on near-copies of the input sequence.\n",
      "\n",
      "**The Transformer processes data in parallel token-wise without knowing all connections upfront, allowing it to handle variable-length contexts and focus softly on relevant parts at any time.**\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40cf038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    " \n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# Define our system and user prompts - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too. \\\n",
    "Finally, show me how to formulate a CV using the information in the website\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbea333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "730328a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model = \"llama3.2\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fda1ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ff3ac17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Craig Rothwell's Resume**\n",
       "\n",
       "Craig Rothwell is a seasoned IT professional and project manager with over 20 years of experience in the technology industry. He holds a Bachelor degree in Mechanical Engineering from University of Limerick and a Project Management Professional (PMP) certification.\n",
       "\n",
       "**Key Highlights**\n",
       "\n",
       "* Experience in business solution architecture, project management, people management, customer/Account Management, technical sales support, and presentation skills\n",
       "* Strong expertise in developing secure and robust applications on AWS technologies\n",
       "* Proven track record of delivering successful projects, including a churning prediction proof-of-concept for a TelisSonera group member\n",
       "\n",
       "**Work Experience**\n",
       "\n",
       "1. **Comptel Oy**, 2012: Program/Project Manager/Solution Architect\n",
       "\t* Managed a project to predict subscriber churn using an analytics-based solution\n",
       "2. **CBOSS Oy**, 2008-2011, 2000-2008: Various roles including Program/Project Manager, Solution Architect, Developer, and Training Manager\n",
       "\t* Performed various tasks such as program management, sales delivery, software engineering, customer data transfer/management, and training product development\n",
       "3. **HP Australia**, 2008-2010: Lead Solution Architect\n",
       "\t* Responsible for architecture, sales support, and delivery of a high-end mission-critical prepay system\n",
       "4. **HP South Pacific**, 2008: HP CMS Account Principle (South Pacific Region)\n",
       "\t* Managed the sale of hardware and software services to Telecom New Zealand and Vodafone Fiji\n",
       "5. **CBOSS Oy**, 2000-2008: Various roles including Software Engineer, Solution Manager, and Training Manager\n",
       "\t* Developed telco service creation environment, customer data transfer/management, Service Delivery Platform functionality\n",
       "\n",
       "**Education**\n",
       "\n",
       "* University of Limerick (1992-1996): Bachelor of Mechanical Engineering majoring in Aircraft Vibrations and Materials\n",
       "* Project Management Institute (PMI) (2013): Project Management Professional (PMP) certification\n",
       "* AWS Certified Solutions Architect – Associate (Ongoing)\n",
       "\n",
       "**Skills**\n",
       "\n",
       "* Business skills: 80%\n",
       "* Business Solution Architecture: 80%\n",
       "* Project Management: 75%\n",
       "* People Management: 50%\n",
       "* Customer/Account Management: 80%\n",
       "* Technical Sales Support: 80%\n",
       "* Presentation Skills: 75%\n",
       "* Technical skills:\n",
       "\t+ HTML5 / CSS3: 70%\n",
       "\t+ JavaScript: 65%\n",
       "\t+ jQuery: 70%\n",
       "\t+ PHP: 80%\n",
       "\t+ AngularJS / Angular 6: 60%\n",
       "\t+ SQL: 75%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://rothwell.fi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
